[
    {
        "Question": "One of your encryption keys stored in Cloud Key Management Service (Cloud KMS) was exposed. You need to re-encrypt all of your CMEK-protected Cloud Storage data that used that key, and then delete the compromised key. You also want to reduce the risk of objects getting written without customer-managed encryption key (CMEK) protection in the future. What should you do?",
        "Possible answers": "A)Rotate the Cloud KMS key version. Continue to use the same Cloud Storage bucket.\nB)Create a new Cloud KMS key. Set the default CMEK key on the existing Cloud Storage bucket to the new one.\nC)Create a new Cloud KMS key. Create a new Cloud Storage bucket. Copy all objects from the old bucket to the new one bucket while specifying the new Cloud KMS key in the copy command.\nD)Create a new Cloud KMS key. Create a new Cloud Storage bucket configured to use the new key as the default CMEK key. Copy all objects from the old bucket to the new bucket without specifying a key.,",
        "Correct answer & Explanation": "Correct Answer:\nD\n\nExplanation:\nA Is not correct because existing data will not automatically be re-encrypted just by rotating the key.\n\nB is not correct because only newly written data will use the new key; existing data will still use the old key.\n\nC is not correct because although it works, similar to the correct answer, but doesn't reduce risk of writing objects without CMEK as requested in the question (since bucket default CMEK key not set).\n\nD Is correct because new bucket with CMEK will ensure that any data subsequently written to it (including the existing data being copied from the old bucket) will be protected with the default key."
    },
    {
        "Question": "You need to connect multiple applications with dynamic public IP addresses to a Cloud SQL instance. You configured users with strong passwords and enforced the SSL connection to your Cloud SQL instance. You want to use Cloud SQL public IP and ensure that you have secured connections. What should you do?",
        "Possible answers": "A)Add all application networks to Authorized Network and regularly update them.\nB)Add CIDR 0.0.0.0/0 network to Authorized Network. Use Identity and Access Management (IAM) to add users.\nC)Add CIDR 0.0.0.0/0 network to Authorized Network. Use Cloud SQL Auth proxy on all applications.\nD)Leave the Authorized Network empty. Use Cloud SQL Auth proxy on all applications.,",
        "Correct answer & Explanation": "Correct Answer:\nD\n\nExplanation:\nD is correct because Cloud SQL Auth proxy allows secured connection for applications with dynamically assigned IP addresses."
    },
    {
        "Question": "You are preparing an organization-wide dataset. You need to preprocess customer data stored in a restricted bucket in Cloud Storage. The data will be used to create consumer analyses. You need to comply with data privacy requirements. What should you do?",
        "Possible answers": "A)Use Dataflow and Cloud KMS to encrypt sensitive fields and write the encrypted data in BigQuery. Share the encryption key by following the principle of least privilege.\nB)Use the Cloud Data Loss Prevention API and Dataflow to detect and remove sensitive fields from the data in Cloud Storage. Write the filtered data in BigQuery.\nC)Use Dataflow and the Cloud Data Loss Prevention API to mask sensitive data. Write the processed data in BigQuery.\nD)Use customer-managed encryption keys (CMEK) to directly encrypt the data in Cloud Storage. Use federated queries from BigQuery. Share the encryption key by following the principle of least privilege.,",
        "Correct answer & Explanation": "Correct Answer:\nC\n\nExplanation:\nA is not correct because it is not a scalable approach for an organization-wide dataset.\n\nB is not correct because while it is scalable, it is suboptimal as you are losing data that might be used to enrich your analyses.\n\nC is correct because it ensures compliance with data privacy requirements while minimizing the loss of information.\n\nD is not correct because it is not a scalable approach for an organization-wide dataset."
    },
    {
        "Question": "Your organization has two Google Cloud projects, project A and project B. In project A, you have a Pub/Sub topic that receives data from confidential sources. Only the resources in project A should be able to access the data in that topic. You want to ensure that project B and any future project cannot access data in the project A topic. What should you do?",
        "Possible answers": "A)Configure VPC Service Controls in the organization with a perimeter around project A.\nB)Configure VPC Service Controls in the organization with a perimeter around the VPC of project A.\nC)Add firewall rules in project A so only traffic from the VPC in project A is permitted.\nD)Use Identity and Access Management conditions to ensure that only users and service accounts in project A can access resources in project A.,",
        "Correct answer & Explanation": "Correct Answer:\nA\n\nExplanation:\nA is correct because the problem is avoiding data exfiltration from project A, and that's what VPC-SC is for (and Pub/Sub is one of the products supported by VPC-SC).\n\nB is not correct because that would have no effect preventing the access to Pub/Sub, since it is a serverless product.\n\nC is not correct because Pub/Sub is a serverless product, and firewall rules will have no effect.\n\nD is not correct because IAM conditions are not supported in Pub/Sub, and project id is not one of the conditions that can be used. Also, users don't belong to projects, so there is no way to select only users in project A."
    },
    {
        "Question": "You are on the data governance team and are implementing security requirements. You need to encrypt all your data in BigQuery by using an encryption key managed by your team. You must implement a mechanism to generate and store encryption material only on your on-premises hardware security module (HSM). You want to rely on Google managed solutions. What should you do?",
        "Possible answers": "A)Create the encryption key in the on-premises HSM, and import it into a Cloud Key Management Service (Cloud KMS) key. Associate the created Cloud KMS key while creating the BigQuery resources.\nB)Create the encryption key in the on-premises HSM, and import it into Cloud Key Management Service (Cloud HSM) key. Associate the created Cloud HSM key while creating the BigQuery resources.\nC)Create the encryption key in the on-premises HSM and link it to a Cloud External Key Manager (Cloud EKM) key. Associate the created Cloud KMS key while creating the BigQuery resources.\nD)Create the encryption key in the on-premises HSM. Create BigQuery resources and encrypt data while ingesting them into BigQuery.,",
        "Correct answer & Explanation": "Correct Answer:\nC\n\nExplanation:\nA is not correct because keys are exported from the on-prem HSM and imported into Cloud KMS.\n\nB is not correct because keys are exported from the on-prem HSM and imported into Cloud HSM.\n\nC is correct because Cloud EKM lets you create and store encryption keys on an external HSM.\n\nD is not correct because it is not a Google-managed solution."
    },
    {
        "Question": "region, but the data could be stored anywhere in the United States. You need to have a recovery process in place in case of a catastrophic single region failure. You need an approach with a maximum of 15 minutes of data loss (RPO=15 mins). You want to ensure that there is minimal latency when reading the data. What should you do?",
        "Possible answers": "A)First, create a Cloud Storage bucket in the US multi-region. Then run the Dataproc cluster in a zone in the us-central1 region, reading data from the US multi-region bucket. In case of a regional failure, redeploy the Dataproc cluster to the us-central2 region and continue reading from the same bucket.\nB)First, create a dual-region Cloud Storage bucket in the us-central1 and us-south1 regions. Enable turbo replication. Then run the Dataproc cluster in a zone in the us-central1 region, reading from the bucket in the same region. In case of a regional failure, redeploy the Dataproc clusters to the us-south1 region and read from the same bucket.\nC)First, create two regional Cloud Storage buckets, one in the us-central1 region and one in the us-south1 region. Have the upstream process write data to the us-central1 bucket. Use the Storage Transfer Service to copy data hourly from the us-central1 bucket to the us-south1 bucket. Then run the Dataproc cluster in a zone in the us-central1 region, reading from the bucket in that region. In case of regional failure, redeploy your Dataproc clusters to the us-south1 region and read from the bucket in that region instead.\nD)First, create a dual-region Cloud Storage bucket in the us-central1 and us-south1 regions. Enable turbo replication. Then run the Dataproc cluster in a zone in the us-central1 region, reading from the bucket in the us-south1 region. In case of a regional failure, redeploy your Dataproc cluster to the us-south1 region and continue reading from the same bucket.,",
        "Correct answer & Explanation": "Correct Answer:\nB\n\nExplanation:\nB is correct because dual-region buckets with turbo-replication have an RPO of 15 mins as required. Dataproc cluster gets redeployed to the available region and is colocated with the bucket."
    },
    {
        "Question": "You maintain ETL pipelines. You notice that a streaming pipeline running on Dataflow is taking a long time to process incoming data, which causes output delays. You also noticed that the pipeline graph was automatically optimized by Dataflow and merged into one step. You want to identify where the potential bottleneck is occurring. What should you do?",
        "Possible answers": "A)Insert output sinks after each key processing step, and observe the writing throughput of each block.\nB)Log debug information in each ParDo function, and analyze the logs at execution time.\nC)Insert a Reshuffle operation after each processing step, and monitor the execution details in the Dataflow console.\nD)Verify that the Dataflow service accounts have appropriate permissions to write the processed data to the output sinks.,",
        "Correct answer & Explanation": "Correct Answer:\nC\n\nExplanation:\nA is not correct as it would create a lot of unnecessary output and it is not really a practical approach to identify processing bottlenecks in the pipeline.\n\nB is not correct as this approach might create an unnecessary amount of logs and it does not really make it easier to identify the step that is causing the bottleneck.\n\nC is correct as the Reshuffle operation prevents the fusion of steps allowing us to observe the throughput of each individual step of the Dataflow pipeline.\n\nD is not correct as the issue is related to the processing of incoming data and not on the writing of output data."
    },
    {
        "Question": "You need to set up a new architecture in Google Cloud that can handle your Hadoop workloads and requires minimal changes to your existing orchestration processes. What should you do?",
        "Possible answers": "A)Use Dataproc to migrate Hadoop clusters to Google Cloud, and Cloud Storage to handle any HDFS use cases. Convert your ETL pipelines to Dataflow.\nB)Use Dataproc to migrate Hadoop clusters to Google Cloud, and Cloud Storage to handle any HDFS use cases. Orchestrate your pipelines with Cloud Composer.\nC)Use Bigtable for your large workloads, with connections to Cloud Storage to handle any HDFS use cases. Orchestrate your pipelines with Cloud Composer.\nD)Use Dataproc to migrate your Hadoop clusters to Google Cloud, and Cloud Storage to handle any HDFS use cases. Use Cloud Data Fusion to visually design and deploy your ETL pipelines.,",
        "Correct answer & Explanation": "Correct Answer:\nB\n\nExplanation:\nA is not correct because in this scenario we are not looking to convert ETL pipelines, but orchestrate them within the Cloud. Dataflow is not an orchestration tool.\n\nB is correct because we would want to convert this company’s Hadoop workloads to Dataproc, and utilize Cloud Storage for their HDFS data. Additionally, Composer would be a great fit for moving their Airflow orchestration on to the cloud.\n\nC is not correct because while Bigtable is a great choice for large analytical and operational workloads, and would be able to hold this organization’s many large datasets, Bigtable is more for an HBase migration, i.e. NOSQL database migration. Therefore this answer wouldn’t solve for migrating Hadoop clusters to the cloud. Bigtable could easily be part of the solution, but there was no indication that the organization was interested in a database move.\n\nD is not correct because although this solution does accurately choose Dataproc and Cloud Storage, Cloud Data Fusion would not be the right choice for orchestration, as it is not an orchestration tool and the scenario did not have any requirements to translate their ETL pipelines into a different product or to do so in a code free fashion."
    },
    {
        "Question": "You are designing the architecture of your application to store data in Cloud Storage. Your application consists of pipelines that read data from a Cloud Storage bucket that contains raw data, and write the data to a second bucket after processing. You want to design an architecture with Cloud Storage resources that are capable of being resilient if a Google Cloud regional failure occurs. You want to minimize the recovery point objective (RPO) if a failure occurs, with no impact on applications that use the stored data. What should you do?",
        "Possible answers": "A)Adopt multi-regional Cloud Storage buckets in your architecture.\nB)Adopt two regional Cloud Storage buckets, and update your application to write the output on both buckets.\nC)Adopt a dual-region Cloud Storage bucket, and enable turbo replication in your architecture.\nD)Adopt two regional Cloud Storage buckets, and create a daily task to copy from one bucket to the other.,",
        "Correct answer & Explanation": "Correct Answer:\nC\n\nExplanation:\nC is correct because dual-region Cloud Storage bucket with turbo replication provides the required RPO of 15 minutes and minimal latency."
    },
    {
        "Question": "You are building an ELT solution in BigQuery by using Dataform. You need to perform uniqueness and null value checks on your final tables. What should you do to efficiently integrate these checks into your pipeline?",
        "Possible answers": "A)Build Dataform assertions into your code.\nB)Create Dataplex data quality tasks.\nC)Build BigQuery user-defined functions (UDFs).\nD)Write a Spark-based stored procedure.,",
        "Correct answer & Explanation": "Correct Answer:\nA\n\nExplanation:\nA Is correct as Dataform assertions are used to execute data quality checks within dataform and as part of a job.\n\nB Is not correct because Dataplex tasks are outside your pipeline.\n\nC Is not correct as BigQuery UDFs are not generally used for data quality checks\n\nD Is not correct as BigQuery Spark based stored procedures are not generally used for data quality checks"
    },
    {
        "Question": "You recently deployed several data processing jobs into your Cloud Composer 2 environment. You notice that some tasks are failing in Apache Airflow. On the monitoring dashboard, you see an increase in the total workers memory usage, and there were worker pod evictions. You need to resolve these errors. What should you do? (Choose two)",
        "Possible answers": "A)Increase the directed acyclic graph (DAG) file parsing interval.\nB)Increase the Cloud Composer 2 environment size from medium to large.\nC)Increase the memory available to the Airflow workers.\nD)Increase the memory available to the Airflow triggerer.\nE)Increase the maximum number of workers and reduce worker concurrency.,",
        "Correct answer & Explanation": "Correct Answer:\nE\n\nExplanation:\nA is not correct because changing DAG file parsing interval only affects the Airflow scheduler. The Airflow workers handle the actual work of a DAG.\n\nB is not correct because changing the environment size modifies only the capacities of Cloud Composer’s backend components, such as the Airflow database and the Airflow queue.\n\nC is correct because the majority of issues with worker pod evictions happen because of out-of-memory situations in workers. This is also suggested by the increased total workers memory usage in the monitoring dashboard. Increasing the memory available to the Airflow workers could reduce the memory pressure.\n\nD is not correct because the increase in memory pressure is with the Airflow workers not the triggerer.\n\nE is correct because limiting the number of tasks each Airflow could work on at the same time and spreading them out to other workers could help to reduce memory pressure on each worker."
    },
    {
        "Question": "Your business users need a way to clean and prepare data before using the data for analysis. Your business users are less technically savvy and prefer to work with graphical user interfaces to define their transformations. After the data has been transformed, the business users want to perform their analysis directly in a spreadsheet. You need to recommend a solution that they can use. What should you do?",
        "Possible answers": "A)Use Dataprep to clean the data, and write the results to BigQuery. Analyze the data by using Connected Sheets.\nB)Use Dataprep to clean the data, and write the results to BigQuery. Analyze the data by using Looker Studio.\nC)Use Dataflow to clean the data, and write the results to BigQuery. Analyze the data by using Connected Sheets.\nD)Use Dataflow to clean the data, and write the results to BigQuery. Analyze the data by using Looker Studio.,",
        "Correct answer & Explanation": "Correct Answer:\nA\n\nExplanation:\nA is correct because Dataprep provides a graphical interface for data preparation and Connected Sheets allows analysis directly in a spreadsheet."
    },
    {
        "Question": "You are designing a Dataflow pipeline for a batch processing job. You want to mitigate multiple zonal failures at job submission time. What should you do?",
        "Possible answers": "A)Specify a worker region by using the --region flag.\nB)Create an Eventarc trigger to resubmit the job in case of zonal failure when submitting the job.\nC)Submit duplicate pipelines in two different zones by using the --zone flag.\nD)Set the pipeline staging location as a regional Cloud Storage bucket.,",
        "Correct answer & Explanation": "Correct Answer:\nA\n\nExplanation:\nA is correct because by specifying a region instead of a specific zone, Dataflow can automatically choose the best possible zone for the job, providing an additional level of fault tolerance.\n\nB is not correct because even though retrying the job submission (without explicitly specifying a zone) can be a helpful step in mitigating zonal failures, it is not optimal as it will add additional delays and overhead.\n\nC is not correct because even though this approach would mitigate zonal failures, duplicated pipelines would cause an additional overhead in terms of costs and management of resources. Also, spinning pipelines in 2 different zones does not fully mitigate failure in case both zones are down while other zones in the region might still be available.\n\nD is not correct because setting the staging location to a regional bucket does not address the risk of workers failures in case of zonal outages."
    },
    {
        "Question": "Ask each team to create authorized views of their data. Grant the biquery.jobUser role to each team.",
        "Possible answers": "A)Specify a worker region by using the --region flag.\nB)Ask each team to publish their data in BigQuery sharing (Analytics Hub). Direct the other teams to subscribe to them.\nC)Create a BigQuery scheduled query to replicate all customer data into team projects.\nD)Enable each team to create materialized views of the data they need to access in their projects.,",
        "Correct answer & Explanation": "Correct Answer:\nB\n\nExplanation:\nA Is not correct because biquery.jobUser does not grant access to data. Authorized views are not minimizing operational efforts needed.\n\nB Is correct because it automatically creates an authorized dataset at subscription time.\n\nC Is not correct because it is increasing costs (data duplication) and adding operational tasks.\n\nD Is not correct because it is increasing costs (data duplication) and adding operational tasks."
    },
    {
        "Question": "You are part of a healthcare organization where data is organized and managed by respective data owners in various storage services. As a result of this decentralized ecosystem, discovering and managing data has become difficult. You need to quickly identify and implement a cost-optimized solution to assist your organization with the following:&nbsp;",
        "Possible answers": "A)Build a new data discovery tool on Google Kubernetes Engine that helps with new source onboarding and data lineage tracking.\nB)Use BigLake to convert the current solution into a data lake architecture.\nC)Use Dataplex to manage data, track data lineage, and perform data quality validation.\nD)Use BigQuery to track data lineage, and use Dataprep to manage data and perform data quality validation.,",
        "Correct answer & Explanation": "Correct Answer:\nC\n\nExplanation:\nC is correct because Dataplex gives us the benefit of managing decentralized ecosystems, do data validations and track lineage"
    },
    {
        "Question": "You want to store your team’s shared tables in a single dataset to make data easily accessible to various analysts. You want to make this data readable but unmodifiable by analysts. At the same time, you want to provide the analysts with individual workspaces in the same project, where they can create and store tables for their own use, without the tables being accessible by other analysts. What should you do?",
        "Possible answers": "A)Give analysts the BigQuery Data Viewer role at the project level. Create one other dataset, and give the analysts the BigQuery Data Editor role on that dataset.\nB)Give analysts the BigQuery Data Viewer role at the project level. Create a dataset for each analyst, and give each analyst the BigQuery Data Editor role at the project level.\nC)Give analysts the BigQuery Data Viewer role on the shared dataset. Create a dataset for each analyst, and give each analyst the BigQuery Data Editor role at the dataset level for their assigned dataset.\nD)Give analysts the BigQuery Data Viewer role on the shared dataset. Create one other dataset and give the analysts the BigQuery Data Editor role on that dataset.,",
        "Correct answer & Explanation": "Correct Answer:\nC\n\nExplanation:\nC is correct because it grants BigQuery Data Viewer at dataset level (not project), creates one dataset per analyst, and grants BigQuery Editor at dataset level."
    },
    {
        "Question": "You have several different file type data sources, such as Apache Parquet and CSV. You want to store the data in Cloud Storage. You need to set up an object sink for your data that allows you to use your own encryption keys. You want to use a GUI-based solution. What should you do?",
        "Possible answers": "A)Use Storage Transfer Service to move files into Cloud Storage.\nB)Use Dataflow to move files into Cloud Storage.\nC)Use Cloud Data Fusion to move files into Cloud Storage.\nD)Use BigQuery Data Transfer Service to move files into BigQuery.,",
        "Correct answer & Explanation": "Correct Answer:\nC\n\nExplanation:\nC is correct because Data Fusion has connectors that can be used to support customer managed keys, and is one of Google Cloud’s top ETL tools, making it a great option to move load files into Cloud Storage. Additionally, Data Fusion supports code free ETL solutioning and is the closest product to Informatica functionality in the answers.\n\nD is not correct because while BigQuery Data Transfer Service does support customer managed encryption key and is a low/no code option, however it is not really the best option for a file sink. Cloud Storage is the better option as an object store."
    },
    {
        "Question": "You are designing a real-time system for a ride hailing app that identifies areas with high demand for rides to effectively reroute available drivers to meet the demand. The system ingests data from multiple sources to Pub/Sub, processes the data, and stores the results for visualization and analysis in real-time dashboards. The data sources include driver location updates every 5 seconds and app-based booking events from riders. The data processing involves real-time aggregation of supply and demand data for the last 30 seconds, every 2 seconds, and storing the results in a low-latency system for visualization. What should you do?",
        "Possible answers": "A)Group the data by using a tumbling window in a Dataflow pipeline, and write the aggregated data to Memorystore.\nB)Group the data by using a hopping window in a Dataflow pipeline, and write the aggregated data to Memorystore.\nC)Group the data by using a session window in a Dataflow pipeline, and write the aggregated data to BigQuery.\nD)Group the data by using a hopping window in a Dataflow pipeline, and write the aggregated data to BigQuery.,",
        "Correct answer & Explanation": "Correct Answer:\nB\n\nExplanation:\nA Is not correct because the tumbling window function is not ideal for real-time processing as it creates fixed, non-overlapping windows which can lead to outdated information in a real-time dashboards\n\nB is correct because the hopping window function is suitable for the given scenario to aggregate for the past 30 seconds for every 2 seconds period. Storing the aggregate data in Cloud Memorystore ensures low latency access to the data which is essential for real time dashboards.\n\nC Is not correct because the session window function is not suitable for this scenario as it is meant to group data based on periods of activity.\n\nD is not correct because BigQuery is not meant for low latency access."
    },
    {
        "Question": "You have terabytes of customer behavioral data streaming from Google Analytics into BigQuery daily. Your customers’ information, such as their preferences, is hosted on a Cloud SQL for MySQL database. Your CRM database is hosted on a Cloud SQL for PostgreSQL instance. The marketing team wants to use your customers’ information from the two databases and the customer behavioral data to create marketing campaigns for yearly active customers. You need to ensure that the marketing team can run the campaigns over 100 times a day on typical days and up to 300 during sales. At the same time you want to keep the load on the Cloud SQL databases to a minimum. What should you do?",
        "Possible answers": "A)Create BigQuery connections to both Cloud SQL databases. Use BigQuery federated queries on the two databases and the Google Analytics data on BigQuery to run these queries.\nB)Create a Dataproc cluster with Trino to establish connections to both Cloud SQL databases and BigQuery, to execute the queries.\nC)Create streams in Datastream to replicate the required tables from both Cloud SQL databases to BigQuery for these queries.\nD)Create a job on Apache Spark with Dataproc Serverless to query both Cloud SQL databases and the Google Analytics data on BigQuery for these queries.,",
        "Correct answer & Explanation": "Correct Answer:\nC\n\nExplanation:\nA is not correct because using BigQuery federated queries will increase the load to the databases significantly as the queries for these queries involve full table scan.\n\nB is not correct because using Trino on Dataproc might require a large cluster to query the data in BigQuery which is not going to be performant loading large amount of data from BigQuery alongside with the data from 2 databases, therefore query execution should be pushed down to BigQuery as much as possible. Does not meet the requirement of keeping load to Cloud SQL databases to a minimum.\n\nC is correct because the required data that reside on the transactional databases are replicated in BigQuery which will minimize the load on the transactional databases as it is handled by change data capture and allow efficient queries for the campaigns queries.\n\nD is not correct because using serverless Spark might require high amount of Data Compute Unit or Shuffle storage to query the data in BigQuery (using Storage Read API) which is not going to be performant and will be costly to maintain, therefore query execution should be pushed down to BigQuery as much as possible. Does not meet the requirement of keeping load to Cloud SQL databases to a minimum."
    },
    {
        "Question": "You have an Oracle database deployed in a VM as part of a Virtual Private Cloud (VPC) network. You want to replicate and continuously synchronize 50 tables to BigQuery. You want to minimize the need to manage infrastructure. What should you do?",
        "Possible answers": "A)Deploy Apache Kafka in the same VPC network, use Kafka Connect Oracle change data capture (CDC), and the Kafka Connect Google BigQuery Sink Connector.\nB)Deploy Apache Kafka in the same VPC network, use Kafka Connect Oracle Change Data Capture (CDC), and Dataflow to stream the Kafka topic to BigQuery.\nC)Create a Datastream service from Oracle to BigQuery, use a private connectivity configuration to the same VPC network, and a connection profile to BigQuery.\nD)Create a Pub/Sub subscription to write to BigQuery directly. Deploy the Debezium Oracle connector to capture changes in the Oracle database, and sink to the Pub/Sub topic.,",
        "Correct answer & Explanation": "Correct Answer:\nC\n\nExplanation:\nA Is not correct because Apache Kafka requires a VM to manage.\n\nB Is not correct because Apache Kafka requires a VM to manage.\n\nC is correct because Datastream is a serverless CDC service.\n\nD is not correct because the Debezium connector requires a VM to manage."
    },
    {
        "Question": "You are using Workflows to call an API that returns a 1KB JSON response, apply some complex business logic on this response, wait for the logic to complete, and then perform a load from a Cloud Storage file to BigQuery. The Workflows standard library does not have sufficient capabilities to perform your complex logic, and you want to use Python's standard library instead. You want to optimize your workflow for simplicity and speed of execution. What should you do?",
        "Possible answers": "A)Create a Dataproc cluster, and use PySpark to apply the logic on your JSON file.\nB)Invoke a Cloud Run function instance that uses Python to apply the logic on your JSON file.\nC)Invoke a subworkflow in Workflows to apply the logic on your JSON file.\nD)Create a Cloud Composer environment and run the logic in Cloud Composer.,",
        "Correct answer & Explanation": "Correct Answer:\nB\n\nExplanation:\nA is not correct because creating a Dataproc cluster to process a 1KB JSON file is not optimized for speed.\n\nB is correct because a Cloud Run function is fast to spin up and supports the use of Python.\n\nC is not correct because creating a subworkflow will only give you access to Workflows standard library, hence not solving the problem\n\nD is not correct because it is not optimizing for simplicity and would be overkill"
    },
    {
        "Question": "You need to look at BigQuery data from a specific table multiple times a day. The underlying table you are querying is several petabytes in size, but you want to filter your data and provide simple aggregations to downstream users. You want to run queries faster and get up-to-date insights quicker. What should you do?",
        "Possible answers": "A)Run a scheduled query to pull the necessary data at specific intervals daily.\nB)Use a cached query to accelerate time to results.\nC)Limit the query columns being pulled in the final result.\nD)Create a materialized view based off of the query being run.,",
        "Correct answer & Explanation": "Correct Answer:\nD\n\nExplanation:\nA is not correct because running a scheduled query that is identical to the existing query won’t really improve anything. You could try to make sure that the scheduled queries are run before the downstream consumer needs it, but this is very rigid and inflexible because from the scenario above we don’t know the exact times the data will be needed. Additionally, running a scheduled query on PBs of data is not very performant, and updates would only occur when queries are run.\n\nB is not correct because since query caching is, in theory, already enabled, this would not speed up the query.\n\nC is not correct because while this is a way to optimize your queries, from the problem statement we have no indication that there are excess columns being brought in by the query. Taking a route that would eliminate data that may be necessary to end consumers is not the best option in this scenario.\n\nD is the correct option in this scenario. Materialized views are optimal for improving query performance if you frequently need to pre-aggregate, pre-filter, pre-join, and recluster data. Since we want to pre-filter PBs of data and take advantage of the performance and incremental refresh of materialized views, this is the best option. Additionally, materialized views will be updated whenever there are changes to the source table, providing fresh data."
    },
    {
        "Question": "You are administering a BigQuery on-demand environment. Your business intelligence tool is submitting hundreds of queries each day that aggregate a large (50 TB) sales history fact table at the day and month levels. These queries have a slow response time and are exceeding cost expectations. You need to decrease response time, lower query costs, and minimize maintenance. What should you do?",
        "Possible answers": "A)Build materialized views on top of the sales table to aggregate data at the day and month level.\nB)Build authorized views on top of the sales table to aggregate data at the day and month level.\nC)Enable BI Engine and add your sales table as a preferred table.\nD)Create a scheduled query to build sales day and sales month aggregate tables on an hourly basis.,",
        "Correct answer & Explanation": "Correct Answer:\nA\n\nExplanation:\nA Is correct as materialized views pre-aggregate data so queries do not scan as much data and run faster\n\nB Is not correct as authorized views are not a performance improvement\n\nC is not correct as BI Engine caches frequently accessed dimension tables and won’t hold a 50 TB table in memory.\n\nD Is not correct because you have to maintain the scheduled queries"
    },
    {
        "Question": "You designed a data warehouse in BigQuery to analyze sales data. You want a self-serving, low-maintenance, and cost-effective solution to share the sales dataset to other business units in your organization. What should you do?",
        "Possible answers": "A)Use the BigQuery Data Transfer Service to create a schedule that copies the sales dataset to the other business units’ projects.\nB)Create and share views with the users in the other business units.\nC)Create a BigQuery sharing (Analytics Hub) private exchange, and publish the sales dataset.\nD)Enable the other business units’ projects to access the authorized views of the sales dataset.,",
        "Correct answer & Explanation": "Correct Answer:\nC\n\nExplanation:\nC is correct because it is cost effective, as the data is not copied and low maintenance as the data can be discovered and subscribed without additional involvement of the data producer."
    },
    {
        "Question": "You orchestrate ETL pipelines by using Cloud Composer. One of the tasks in the Airflow DAG relies on a third-party service. You want to be notified when the task does not succeed. What should you do?",
        "Possible answers": "A)Assign a function with notification logic to the on_failure_callback parameter for the operator responsible for the task at risk.\nB)Assign a function with notification logic to the sla_miss_callback parameter for the operator responsible for the task at risk.\nC)Assign a function with notification logic to the on_retry_callback parameter for the operator responsible for the task at risk.\nD)Configure a Cloud Monitoring alert on the sla_missed metric associated with the task at risk to trigger a notification.,",
        "Correct answer & Explanation": "Correct Answer:\nA\n\nExplanation:\nA is correct as the callback function will be invoked when the specific task fails.\n\nB is not correct as the callback function will be invoked when the task misses its defined SLAs and not specifically when it fails.\n\nC is not correct as the callback function will be invoked when the task is retried after a failure and not if it just fails without retrying.\n\nD is not correct as the alert is set up on the wrong metric and it requires additional configurations."
    },
    {
        "Question": "You need to create a SQL pipeline. The pipeline runs an aggregate SQL transformation on a BigQuery table every two hours and appends the result to another existing BigQuery table. You need to configure the pipeline to retry if errors occur. You want the pipeline to send an email notification after three consecutive failures. What should you do?",
        "Possible answers": "A)Create a BigQuery scheduled query to run the SQL transformation with schedule options that repeats every two hours, and enable email notifications.\nB)Use the BigQueryUpsertTableOperator in Cloud Composer, set the retry parameter to three, and set the email_on_failure parameter to true.\nC)Create a BigQuery scheduled query to run the SQL transformation with schedule options that repeats every two hours, and enable notification to Pub/Sub topic. Use Pub/Sub and Cloud Run functions to send an email after three failed executions.\nD)Use the BigQueryInsertJobOperator in Cloud Composer, set the retry parameter to three, and set the email_on_failure parameter to true.,",
        "Correct answer & Explanation": "Correct Answer:\nD\n\nExplanation:\nA is not correct because BigQuery scheduled query does not have retry options.\n\nB is not correct because BigQueryUpsertTableOperator is used to create/update table properties and not to run SQL transformation.\n\nC is not correct because BigQuery scheduled query does not have retry options even though custom notification logic can be built as a Cloud Run function.\n\nD is correct because the BigQueryInsertJobOperator operator can run sql transformation queries on a BigQuery table and append the results to another BigQuery table. With retry=3 and email_on_failure=True, the operator retries 3 times and then sends email notification if all the 3 retries are exhausted"
    }
]